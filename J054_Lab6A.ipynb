{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J054_Lab6A.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narayananv10/DeepLearning/blob/master/J054_Lab6A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmQKx4diH0vV",
        "colab_type": "text"
      },
      "source": [
        "#Usage of activations\n",
        "\n",
        "Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from keras.layers import Activation, Dense\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('tanh'))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "This is equivalent to:\n",
        "\n",
        "```\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "```\n",
        "\n",
        "You can also pass an element-wise TensorFlow/Theano/CNTK function as an activation:\n",
        "\n",
        "```\n",
        "from keras import backend as K\n",
        "model.add(Dense(64, activation=K.tanh))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcMWdTDBIik-",
        "colab_type": "text"
      },
      "source": [
        "#elu\n",
        "\n",
        "keras.activations.elu(x, alpha=1.0)\n",
        "\n",
        "Exponential linear unit.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "2. alpha: A scalar, slope of negative section.\n",
        "\n",
        "Returns\n",
        "\n",
        "1. The exponential linear activation: x if x > 0 and alpha * (exp(x)-1) if x < 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFwOV5GJHUoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='elu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='elu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUN3hrzoKAmX",
        "colab_type": "text"
      },
      "source": [
        "#softmax\n",
        "\n",
        "keras.activations.softmax(x, axis=-1)\n",
        "\n",
        "##Softmax activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "2. axis: Integer, axis along which the softmax normalization is applied.\n",
        "\n",
        "##Returns\n",
        "\n",
        "Tensor, output of softmax transformation.\n",
        "\n",
        "##Raises\n",
        "\n",
        "ValueError: In case dim(x) == 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOQfshRuKK2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='softmax', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='softmax'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM19cjk8KTL7",
        "colab_type": "text"
      },
      "source": [
        "#selu\n",
        "\n",
        "keras.activations.selu(x)\n",
        "\n",
        "Scaled Exponential Linear Unit (SELU).\n",
        "\n",
        "SELU is equal to: scale * elu(x, alpha), where alpha and scale are predefined constants. The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see lecun_normal initialization) and the number of inputs is \"large enough\" (see references for more information).\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: A tensor or variable to compute the activation function for.\n",
        "\n",
        "##Returns\n",
        "\n",
        "The scaled exponential unit activation: scale * elu(x, alpha).\n",
        "\n",
        "##Note\n",
        "\n",
        "1. To be used together with the initialization \"lecun_normal\".\n",
        "2. To be used together with the dropout variant \"AlphaDropout\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqueXLpaKddp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='selu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='selu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jh93vQKKjws",
        "colab_type": "text"
      },
      "source": [
        "#softplus\n",
        "\n",
        "keras.activations.softplus(x)\n",
        "\n",
        "Softplus activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The softplus activation: log(exp(x) + 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msVEMJUtKwTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='softplus', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='softplus'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5us_flAJK4lP",
        "colab_type": "text"
      },
      "source": [
        "#softsign\n",
        "\n",
        "keras.activations.softsign(x)\n",
        "\n",
        "##Softsign activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The softsign activation: x / (abs(x) + 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxsE4bDzK_Wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='softsign', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='softsign'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1frWDGbLIPD",
        "colab_type": "text"
      },
      "source": [
        "#relu\n",
        "\n",
        "keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)\n",
        "\n",
        "##Rectified Linear Unit.\n",
        "\n",
        "With default values, it returns element-wise max(x, 0).\n",
        "\n",
        "Otherwise, it follows: f(x) = max_value for x >= max_value, f(x) = x for threshold <= x < max_value, f(x) = alpha * (x - threshold) otherwise.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "2. alpha: float. Slope of the negative part. Defaults to zero.\n",
        "3. max_value: float. Saturation threshold.\n",
        "4. threshold: float. Threshold value for thresholded activation.\n",
        "\n",
        "##Returns\n",
        "\n",
        "A tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-yB_H-6LEmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb6436cKLXyP",
        "colab_type": "text"
      },
      "source": [
        "#tanh\n",
        "\n",
        "keras.activations.tanh(x)\n",
        "\n",
        "##Hyperbolic tangent activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The hyperbolic activation: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l_JVgdfLd_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='tanh', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwPdvFC3LmME",
        "colab_type": "text"
      },
      "source": [
        "#sigmoid\n",
        "\n",
        "keras.activations.sigmoid(x)\n",
        "\n",
        "##Sigmoid activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. The sigmoid activation: 1 / (1 + exp(-x))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chkhKBK4LtS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='sigmoid', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDSa6h2iL016",
        "colab_type": "text"
      },
      "source": [
        "#hard_sigmoid\n",
        "\n",
        "keras.activations.hard_sigmoid(x)\n",
        "\n",
        "##Hard sigmoid activation function.\n",
        "\n",
        "Faster to compute than sigmoid activation.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. Hard sigmoid activation:\n",
        "\n",
        "```\n",
        "0 if x < -2.5\n",
        "1 if x > 2.5\n",
        "0.2 * x + 0.5 if -2.5 <= x <= 2.5.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPIE0vA2MIY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='hard_sigmoid', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='hard_sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkEibscrMTJ7",
        "colab_type": "text"
      },
      "source": [
        "#exponential\n",
        "\n",
        "keras.activations.exponential(x)\n",
        "\n",
        "Exponential (base e) activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "1. Exponential activation: exp(x)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z03_Vua1MY82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='exponential', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='exponential'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twHrIcibMfqj",
        "colab_type": "text"
      },
      "source": [
        "#linear\n",
        "\n",
        "keras.activations.linear(x)\n",
        "\n",
        "Linear (i.e. identity) activation function.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. x: Input tensor.\n",
        "\n",
        "##Returns\n",
        "\n",
        "Input tensor, unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4yIoO3RMk-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='linear', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='linear'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OyPjtogMtdG",
        "colab_type": "text"
      },
      "source": [
        "#LeakyReLU\n",
        "\n",
        "keras.layers.LeakyReLU(alpha=0.3)\n",
        "\n",
        "Leaky version of a Rectified Linear Unit.\n",
        "\n",
        "It allows a small gradient when the unit is not active: \n",
        "```\n",
        "f(x) = alpha * x for x < 0\n",
        "f(x) = x for x >= 0.\n",
        "```\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "alpha: float >= 0. Negative slope coefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erjq0UrxNRum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.LeakyReLU(alpha=0.3), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.LeakyReLU(alpha=0.3)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJBHILnNv8k",
        "colab_type": "text"
      },
      "source": [
        "#PReLU\n",
        "\n",
        "keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
        "\n",
        "Parametric Rectified Linear Unit.\n",
        "\n",
        "It follows: f(x) = alpha * x for x < 0, f(x) = x for x >= 0, where alpha is a learned array with the same shape as x.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. alpha_initializer: initializer function for the weights.\n",
        "2. alpha_regularizer: regularizer for the weights.\n",
        "3. alpha_constraint: constraint for the weights.\n",
        "4. shared_axes: the axes along which to share learnable parameters for the activation function. For example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels), and you wish to share parameters across space so that each filter only has one set of parameters, set shared_axes=[1, 2]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sMj5qFNN7Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
        ", input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
        "))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZFbSZYOSCi",
        "colab_type": "text"
      },
      "source": [
        "#ELU\n",
        "\n",
        "keras.layers.ELU(alpha=1.0)\n",
        "\n",
        "Exponential Linear Unit.\n",
        "\n",
        "It follows: f(x) =  alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. alpha: scale for the negative factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV_7KHrmOZEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.ELU(alpha=1.0), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.ELU(alpha=1.0)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMMit6nZOlXi",
        "colab_type": "text"
      },
      "source": [
        "#ThresholdedReLU\n",
        "\n",
        "keras.layers.ThresholdedReLU(theta=1.0)\n",
        "\n",
        "Thresholded Rectified Linear Unit.\n",
        "\n",
        "It follows: f(x) = x for x > theta, f(x) = 0 otherwise.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. theta: float >= 0. Threshold location of activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtN3bUOOOqN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.ThresholdedReLU(theta=1.0), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.ThresholdedReLU(theta=1.0)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHf3ajyEOy7R",
        "colab_type": "text"
      },
      "source": [
        "#Softmax\n",
        "\n",
        "keras.layers.Softmax(axis=-1)\n",
        "\n",
        "Softmax activation function.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. axis: Integer, axis along which the softmax normalization is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSzMbFfGO600",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.Softmax(axis=-1), input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.Softmax(axis=-1)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCbXH5M2PCwO",
        "colab_type": "text"
      },
      "source": [
        "#ReLU\n",
        "\n",
        "keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "\n",
        "Rectified Linear Unit activation function.\n",
        "\n",
        "With default values, it returns element-wise max(x, 0).\n",
        "\n",
        "Otherwise, it follows: f(x) = max_value for x >= max_value, f(x) = x for threshold <= x < max_value, f(x) = negative_slope * (x - threshold) otherwise.\n",
        "\n",
        "##Input shape\n",
        "\n",
        "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "\n",
        "##Output shape\n",
        "\n",
        "Same shape as the input.\n",
        "\n",
        "##Arguments\n",
        "\n",
        "1. max_value: float >= 0. Maximum activation value.\n",
        "2. negative_slope: float >= 0. Negative slope coefficient.\n",
        "3. threshold: float. Threshold value for thresholded activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMee8FeqPK2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        ", input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation=keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
        "))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw_XEgUlPU04",
        "colab_type": "text"
      },
      "source": [
        "#Thank you for completing this notebook"
      ]
    }
  ]
}